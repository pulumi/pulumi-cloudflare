// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Cloudflare
{
    /// <summary>
    /// ## Import
    /// 
    /// Logpush jobs can be imported using a composite ID formed of* `identifierType` - Either `account` or `zone`. * `identifierID` - The ID of the account or zone. * `jobID` - The Logpush Job ID to import. Import an account-scoped job using `account/:accountID/:jobID`
    /// 
    /// ```sh
    ///  $ pulumi import cloudflare:index/logpushJob:LogpushJob example account/1d5fdc9e88c8a8c4518b068cd94331fe/54321
    /// ```
    /// 
    ///  Import a zone-scoped job using `zone/:zoneID/:jobID`
    /// 
    /// ```sh
    ///  $ pulumi import cloudflare:index/logpushJob:LogpushJob example zone/d41d8cd98f00b204e9800998ecf8427e/54321
    /// ```
    /// </summary>
    [CloudflareResourceType("cloudflare:index/logpushJob:LogpushJob")]
    public partial class LogpushJob : Pulumi.CustomResource
    {
        /// <summary>
        /// The account ID where the logpush job should be created. Either `account_id` or `zone_id` are required.
        /// </summary>
        [Output("accountId")]
        public Output<string?> AccountId { get; private set; } = null!;

        /// <summary>
        /// Which type of dataset resource to use. Available values are
        /// - [account-scoped](https://developers.cloudflare.com/logs/reference/log-fields/account): `"audit_logs"`, `"gateway_dns"`, `"gateway_http"`, `"gateway_network"`
        /// - [zone-scoped](https://developers.cloudflare.com/logs/reference/log-fields/zone): `"firewall_events"`, `"http_requests"`, `"spectrum_events"`, `"nel_reports"`
        /// </summary>
        [Output("dataset")]
        public Output<string> Dataset { get; private set; } = null!;

        /// <summary>
        /// Uniquely identifies a resource (such as an s3 bucket) where data will be pushed. Additional configuration parameters supported by the destination may be included. See [Logpush destination documentation](https://developers.cloudflare.com/logs/reference/logpush-api-configuration#destination).
        /// </summary>
        [Output("destinationConf")]
        public Output<string> DestinationConf { get; private set; } = null!;

        /// <summary>
        /// Whether to enable the job.
        /// </summary>
        [Output("enabled")]
        public Output<bool?> Enabled { get; private set; } = null!;

        /// <summary>
        /// Configuration string for the Logshare API. It specifies things like requested fields and timestamp formats. See [Logpull options documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#options).
        /// </summary>
        [Output("logpullOptions")]
        public Output<string?> LogpullOptions { get; private set; } = null!;

        /// <summary>
        /// The name of the logpush job to create. Must match the regular expression `^[a-zA-Z0-9\-\.]*$`.
        /// </summary>
        [Output("name")]
        public Output<string?> Name { get; private set; } = null!;

        /// <summary>
        /// Ownership challenge token to prove destination ownership, required when destination is Amazon S3, Google Cloud Storage,
        /// Microsoft Azure or Sumo Logic. See [Developer documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#usage).
        /// </summary>
        [Output("ownershipChallenge")]
        public Output<string?> OwnershipChallenge { get; private set; } = null!;

        /// <summary>
        /// The zone ID where the logpush job should be created. Either `account_id` or `zone_id` are required.
        /// </summary>
        [Output("zoneId")]
        public Output<string?> ZoneId { get; private set; } = null!;


        /// <summary>
        /// Create a LogpushJob resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public LogpushJob(string name, LogpushJobArgs args, CustomResourceOptions? options = null)
            : base("cloudflare:index/logpushJob:LogpushJob", name, args ?? new LogpushJobArgs(), MakeResourceOptions(options, ""))
        {
        }

        private LogpushJob(string name, Input<string> id, LogpushJobState? state = null, CustomResourceOptions? options = null)
            : base("cloudflare:index/logpushJob:LogpushJob", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing LogpushJob resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static LogpushJob Get(string name, Input<string> id, LogpushJobState? state = null, CustomResourceOptions? options = null)
        {
            return new LogpushJob(name, id, state, options);
        }
    }

    public sealed class LogpushJobArgs : Pulumi.ResourceArgs
    {
        /// <summary>
        /// The account ID where the logpush job should be created. Either `account_id` or `zone_id` are required.
        /// </summary>
        [Input("accountId")]
        public Input<string>? AccountId { get; set; }

        /// <summary>
        /// Which type of dataset resource to use. Available values are
        /// - [account-scoped](https://developers.cloudflare.com/logs/reference/log-fields/account): `"audit_logs"`, `"gateway_dns"`, `"gateway_http"`, `"gateway_network"`
        /// - [zone-scoped](https://developers.cloudflare.com/logs/reference/log-fields/zone): `"firewall_events"`, `"http_requests"`, `"spectrum_events"`, `"nel_reports"`
        /// </summary>
        [Input("dataset", required: true)]
        public Input<string> Dataset { get; set; } = null!;

        /// <summary>
        /// Uniquely identifies a resource (such as an s3 bucket) where data will be pushed. Additional configuration parameters supported by the destination may be included. See [Logpush destination documentation](https://developers.cloudflare.com/logs/reference/logpush-api-configuration#destination).
        /// </summary>
        [Input("destinationConf", required: true)]
        public Input<string> DestinationConf { get; set; } = null!;

        /// <summary>
        /// Whether to enable the job.
        /// </summary>
        [Input("enabled")]
        public Input<bool>? Enabled { get; set; }

        /// <summary>
        /// Configuration string for the Logshare API. It specifies things like requested fields and timestamp formats. See [Logpull options documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#options).
        /// </summary>
        [Input("logpullOptions")]
        public Input<string>? LogpullOptions { get; set; }

        /// <summary>
        /// The name of the logpush job to create. Must match the regular expression `^[a-zA-Z0-9\-\.]*$`.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// Ownership challenge token to prove destination ownership, required when destination is Amazon S3, Google Cloud Storage,
        /// Microsoft Azure or Sumo Logic. See [Developer documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#usage).
        /// </summary>
        [Input("ownershipChallenge")]
        public Input<string>? OwnershipChallenge { get; set; }

        /// <summary>
        /// The zone ID where the logpush job should be created. Either `account_id` or `zone_id` are required.
        /// </summary>
        [Input("zoneId")]
        public Input<string>? ZoneId { get; set; }

        public LogpushJobArgs()
        {
        }
    }

    public sealed class LogpushJobState : Pulumi.ResourceArgs
    {
        /// <summary>
        /// The account ID where the logpush job should be created. Either `account_id` or `zone_id` are required.
        /// </summary>
        [Input("accountId")]
        public Input<string>? AccountId { get; set; }

        /// <summary>
        /// Which type of dataset resource to use. Available values are
        /// - [account-scoped](https://developers.cloudflare.com/logs/reference/log-fields/account): `"audit_logs"`, `"gateway_dns"`, `"gateway_http"`, `"gateway_network"`
        /// - [zone-scoped](https://developers.cloudflare.com/logs/reference/log-fields/zone): `"firewall_events"`, `"http_requests"`, `"spectrum_events"`, `"nel_reports"`
        /// </summary>
        [Input("dataset")]
        public Input<string>? Dataset { get; set; }

        /// <summary>
        /// Uniquely identifies a resource (such as an s3 bucket) where data will be pushed. Additional configuration parameters supported by the destination may be included. See [Logpush destination documentation](https://developers.cloudflare.com/logs/reference/logpush-api-configuration#destination).
        /// </summary>
        [Input("destinationConf")]
        public Input<string>? DestinationConf { get; set; }

        /// <summary>
        /// Whether to enable the job.
        /// </summary>
        [Input("enabled")]
        public Input<bool>? Enabled { get; set; }

        /// <summary>
        /// Configuration string for the Logshare API. It specifies things like requested fields and timestamp formats. See [Logpull options documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#options).
        /// </summary>
        [Input("logpullOptions")]
        public Input<string>? LogpullOptions { get; set; }

        /// <summary>
        /// The name of the logpush job to create. Must match the regular expression `^[a-zA-Z0-9\-\.]*$`.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// Ownership challenge token to prove destination ownership, required when destination is Amazon S3, Google Cloud Storage,
        /// Microsoft Azure or Sumo Logic. See [Developer documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#usage).
        /// </summary>
        [Input("ownershipChallenge")]
        public Input<string>? OwnershipChallenge { get; set; }

        /// <summary>
        /// The zone ID where the logpush job should be created. Either `account_id` or `zone_id` are required.
        /// </summary>
        [Input("zoneId")]
        public Input<string>? ZoneId { get; set; }

        public LogpushJobState()
        {
        }
    }
}
