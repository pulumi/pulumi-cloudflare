# coding=utf-8
# *** WARNING: this file was generated by pulumi-language-python. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import builtins as _builtins
import warnings
import sys
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
if sys.version_info >= (3, 11):
    from typing import NotRequired, TypedDict, TypeAlias
else:
    from typing_extensions import NotRequired, TypedDict, TypeAlias
from . import _utilities
from . import outputs
from ._inputs import *

__all__ = ['AiSearchInstanceArgs', 'AiSearchInstance']

@pulumi.input_type
class AiSearchInstanceArgs:
    def __init__(__self__, *,
                 account_id: pulumi.Input[_builtins.str],
                 ai_search_id: pulumi.Input[_builtins.str],
                 source: pulumi.Input[_builtins.str],
                 type: pulumi.Input[_builtins.str],
                 ai_gateway_id: Optional[pulumi.Input[_builtins.str]] = None,
                 aisearch_model: Optional[pulumi.Input[_builtins.str]] = None,
                 cache: Optional[pulumi.Input[_builtins.bool]] = None,
                 cache_threshold: Optional[pulumi.Input[_builtins.str]] = None,
                 chunk: Optional[pulumi.Input[_builtins.bool]] = None,
                 chunk_overlap: Optional[pulumi.Input[_builtins.int]] = None,
                 chunk_size: Optional[pulumi.Input[_builtins.int]] = None,
                 custom_metadatas: Optional[pulumi.Input[Sequence[pulumi.Input['AiSearchInstanceCustomMetadataArgs']]]] = None,
                 embedding_model: Optional[pulumi.Input[_builtins.str]] = None,
                 hybrid_search_enabled: Optional[pulumi.Input[_builtins.bool]] = None,
                 max_num_results: Optional[pulumi.Input[_builtins.int]] = None,
                 metadata: Optional[pulumi.Input['AiSearchInstanceMetadataArgs']] = None,
                 paused: Optional[pulumi.Input[_builtins.bool]] = None,
                 public_endpoint_params: Optional[pulumi.Input['AiSearchInstancePublicEndpointParamsArgs']] = None,
                 reranking: Optional[pulumi.Input[_builtins.bool]] = None,
                 reranking_model: Optional[pulumi.Input[_builtins.str]] = None,
                 rewrite_model: Optional[pulumi.Input[_builtins.str]] = None,
                 rewrite_query: Optional[pulumi.Input[_builtins.bool]] = None,
                 score_threshold: Optional[pulumi.Input[_builtins.float]] = None,
                 source_params: Optional[pulumi.Input['AiSearchInstanceSourceParamsArgs']] = None,
                 summarization: Optional[pulumi.Input[_builtins.bool]] = None,
                 summarization_model: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_aisearch: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_index_summarization: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_rewrite_query: Optional[pulumi.Input[_builtins.str]] = None,
                 token_id: Optional[pulumi.Input[_builtins.str]] = None):
        """
        The set of arguments for constructing a AiSearchInstance resource.
        :param pulumi.Input[_builtins.str] ai_search_id: Use your AI Search ID.
        :param pulumi.Input[_builtins.str] type: Available values: "r2", "web-crawler".
        :param pulumi.Input[_builtins.str] aisearch_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] cache_threshold: Available values: "super*strict*match", "close*enough", "flexible*friend", "anything_goes".
        :param pulumi.Input[_builtins.str] embedding_model: Available values: "@cf/qwen/qwen3-embedding-0.6b", "@cf/baai/bge-m3", "@cf/baai/bge-large-en-v1.5", "@cf/google/embeddinggemma-300m", "google-ai-studio/gemini-embedding-001", "openai/text-embedding-3-small", "openai/text-embedding-3-large", "".
        :param pulumi.Input[_builtins.str] reranking_model: Available values: "@cf/baai/bge-reranker-base", "".
        :param pulumi.Input[_builtins.str] rewrite_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] summarization_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        pulumi.set(__self__, "account_id", account_id)
        pulumi.set(__self__, "ai_search_id", ai_search_id)
        pulumi.set(__self__, "source", source)
        pulumi.set(__self__, "type", type)
        if ai_gateway_id is not None:
            pulumi.set(__self__, "ai_gateway_id", ai_gateway_id)
        if aisearch_model is not None:
            pulumi.set(__self__, "aisearch_model", aisearch_model)
        if cache is not None:
            pulumi.set(__self__, "cache", cache)
        if cache_threshold is not None:
            pulumi.set(__self__, "cache_threshold", cache_threshold)
        if chunk is not None:
            pulumi.set(__self__, "chunk", chunk)
        if chunk_overlap is not None:
            pulumi.set(__self__, "chunk_overlap", chunk_overlap)
        if chunk_size is not None:
            pulumi.set(__self__, "chunk_size", chunk_size)
        if custom_metadatas is not None:
            pulumi.set(__self__, "custom_metadatas", custom_metadatas)
        if embedding_model is not None:
            pulumi.set(__self__, "embedding_model", embedding_model)
        if hybrid_search_enabled is not None:
            pulumi.set(__self__, "hybrid_search_enabled", hybrid_search_enabled)
        if max_num_results is not None:
            pulumi.set(__self__, "max_num_results", max_num_results)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)
        if paused is not None:
            pulumi.set(__self__, "paused", paused)
        if public_endpoint_params is not None:
            pulumi.set(__self__, "public_endpoint_params", public_endpoint_params)
        if reranking is not None:
            pulumi.set(__self__, "reranking", reranking)
        if reranking_model is not None:
            pulumi.set(__self__, "reranking_model", reranking_model)
        if rewrite_model is not None:
            pulumi.set(__self__, "rewrite_model", rewrite_model)
        if rewrite_query is not None:
            pulumi.set(__self__, "rewrite_query", rewrite_query)
        if score_threshold is not None:
            pulumi.set(__self__, "score_threshold", score_threshold)
        if source_params is not None:
            pulumi.set(__self__, "source_params", source_params)
        if summarization is not None:
            pulumi.set(__self__, "summarization", summarization)
        if summarization_model is not None:
            pulumi.set(__self__, "summarization_model", summarization_model)
        if system_prompt_aisearch is not None:
            pulumi.set(__self__, "system_prompt_aisearch", system_prompt_aisearch)
        if system_prompt_index_summarization is not None:
            pulumi.set(__self__, "system_prompt_index_summarization", system_prompt_index_summarization)
        if system_prompt_rewrite_query is not None:
            pulumi.set(__self__, "system_prompt_rewrite_query", system_prompt_rewrite_query)
        if token_id is not None:
            pulumi.set(__self__, "token_id", token_id)

    @_builtins.property
    @pulumi.getter(name="accountId")
    def account_id(self) -> pulumi.Input[_builtins.str]:
        return pulumi.get(self, "account_id")

    @account_id.setter
    def account_id(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "account_id", value)

    @_builtins.property
    @pulumi.getter(name="aiSearchId")
    def ai_search_id(self) -> pulumi.Input[_builtins.str]:
        """
        Use your AI Search ID.
        """
        return pulumi.get(self, "ai_search_id")

    @ai_search_id.setter
    def ai_search_id(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "ai_search_id", value)

    @_builtins.property
    @pulumi.getter
    def source(self) -> pulumi.Input[_builtins.str]:
        return pulumi.get(self, "source")

    @source.setter
    def source(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "source", value)

    @_builtins.property
    @pulumi.getter
    def type(self) -> pulumi.Input[_builtins.str]:
        """
        Available values: "r2", "web-crawler".
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: pulumi.Input[_builtins.str]):
        pulumi.set(self, "type", value)

    @_builtins.property
    @pulumi.getter(name="aiGatewayId")
    def ai_gateway_id(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "ai_gateway_id")

    @ai_gateway_id.setter
    def ai_gateway_id(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "ai_gateway_id", value)

    @_builtins.property
    @pulumi.getter(name="aisearchModel")
    def aisearch_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        return pulumi.get(self, "aisearch_model")

    @aisearch_model.setter
    def aisearch_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "aisearch_model", value)

    @_builtins.property
    @pulumi.getter
    def cache(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "cache")

    @cache.setter
    def cache(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "cache", value)

    @_builtins.property
    @pulumi.getter(name="cacheThreshold")
    def cache_threshold(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "super*strict*match", "close*enough", "flexible*friend", "anything_goes".
        """
        return pulumi.get(self, "cache_threshold")

    @cache_threshold.setter
    def cache_threshold(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "cache_threshold", value)

    @_builtins.property
    @pulumi.getter
    def chunk(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "chunk")

    @chunk.setter
    def chunk(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "chunk", value)

    @_builtins.property
    @pulumi.getter(name="chunkOverlap")
    def chunk_overlap(self) -> Optional[pulumi.Input[_builtins.int]]:
        return pulumi.get(self, "chunk_overlap")

    @chunk_overlap.setter
    def chunk_overlap(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "chunk_overlap", value)

    @_builtins.property
    @pulumi.getter(name="chunkSize")
    def chunk_size(self) -> Optional[pulumi.Input[_builtins.int]]:
        return pulumi.get(self, "chunk_size")

    @chunk_size.setter
    def chunk_size(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "chunk_size", value)

    @_builtins.property
    @pulumi.getter(name="customMetadatas")
    def custom_metadatas(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['AiSearchInstanceCustomMetadataArgs']]]]:
        return pulumi.get(self, "custom_metadatas")

    @custom_metadatas.setter
    def custom_metadatas(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['AiSearchInstanceCustomMetadataArgs']]]]):
        pulumi.set(self, "custom_metadatas", value)

    @_builtins.property
    @pulumi.getter(name="embeddingModel")
    def embedding_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/qwen/qwen3-embedding-0.6b", "@cf/baai/bge-m3", "@cf/baai/bge-large-en-v1.5", "@cf/google/embeddinggemma-300m", "google-ai-studio/gemini-embedding-001", "openai/text-embedding-3-small", "openai/text-embedding-3-large", "".
        """
        return pulumi.get(self, "embedding_model")

    @embedding_model.setter
    def embedding_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "embedding_model", value)

    @_builtins.property
    @pulumi.getter(name="hybridSearchEnabled")
    def hybrid_search_enabled(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "hybrid_search_enabled")

    @hybrid_search_enabled.setter
    def hybrid_search_enabled(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "hybrid_search_enabled", value)

    @_builtins.property
    @pulumi.getter(name="maxNumResults")
    def max_num_results(self) -> Optional[pulumi.Input[_builtins.int]]:
        return pulumi.get(self, "max_num_results")

    @max_num_results.setter
    def max_num_results(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "max_num_results", value)

    @_builtins.property
    @pulumi.getter
    def metadata(self) -> Optional[pulumi.Input['AiSearchInstanceMetadataArgs']]:
        return pulumi.get(self, "metadata")

    @metadata.setter
    def metadata(self, value: Optional[pulumi.Input['AiSearchInstanceMetadataArgs']]):
        pulumi.set(self, "metadata", value)

    @_builtins.property
    @pulumi.getter
    def paused(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "paused")

    @paused.setter
    def paused(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "paused", value)

    @_builtins.property
    @pulumi.getter(name="publicEndpointParams")
    def public_endpoint_params(self) -> Optional[pulumi.Input['AiSearchInstancePublicEndpointParamsArgs']]:
        return pulumi.get(self, "public_endpoint_params")

    @public_endpoint_params.setter
    def public_endpoint_params(self, value: Optional[pulumi.Input['AiSearchInstancePublicEndpointParamsArgs']]):
        pulumi.set(self, "public_endpoint_params", value)

    @_builtins.property
    @pulumi.getter
    def reranking(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "reranking")

    @reranking.setter
    def reranking(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "reranking", value)

    @_builtins.property
    @pulumi.getter(name="rerankingModel")
    def reranking_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/baai/bge-reranker-base", "".
        """
        return pulumi.get(self, "reranking_model")

    @reranking_model.setter
    def reranking_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "reranking_model", value)

    @_builtins.property
    @pulumi.getter(name="rewriteModel")
    def rewrite_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        return pulumi.get(self, "rewrite_model")

    @rewrite_model.setter
    def rewrite_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "rewrite_model", value)

    @_builtins.property
    @pulumi.getter(name="rewriteQuery")
    def rewrite_query(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "rewrite_query")

    @rewrite_query.setter
    def rewrite_query(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "rewrite_query", value)

    @_builtins.property
    @pulumi.getter(name="scoreThreshold")
    def score_threshold(self) -> Optional[pulumi.Input[_builtins.float]]:
        return pulumi.get(self, "score_threshold")

    @score_threshold.setter
    def score_threshold(self, value: Optional[pulumi.Input[_builtins.float]]):
        pulumi.set(self, "score_threshold", value)

    @_builtins.property
    @pulumi.getter(name="sourceParams")
    def source_params(self) -> Optional[pulumi.Input['AiSearchInstanceSourceParamsArgs']]:
        return pulumi.get(self, "source_params")

    @source_params.setter
    def source_params(self, value: Optional[pulumi.Input['AiSearchInstanceSourceParamsArgs']]):
        pulumi.set(self, "source_params", value)

    @_builtins.property
    @pulumi.getter
    def summarization(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "summarization")

    @summarization.setter
    def summarization(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "summarization", value)

    @_builtins.property
    @pulumi.getter(name="summarizationModel")
    def summarization_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        return pulumi.get(self, "summarization_model")

    @summarization_model.setter
    def summarization_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "summarization_model", value)

    @_builtins.property
    @pulumi.getter(name="systemPromptAisearch")
    def system_prompt_aisearch(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "system_prompt_aisearch")

    @system_prompt_aisearch.setter
    def system_prompt_aisearch(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "system_prompt_aisearch", value)

    @_builtins.property
    @pulumi.getter(name="systemPromptIndexSummarization")
    def system_prompt_index_summarization(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "system_prompt_index_summarization")

    @system_prompt_index_summarization.setter
    def system_prompt_index_summarization(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "system_prompt_index_summarization", value)

    @_builtins.property
    @pulumi.getter(name="systemPromptRewriteQuery")
    def system_prompt_rewrite_query(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "system_prompt_rewrite_query")

    @system_prompt_rewrite_query.setter
    def system_prompt_rewrite_query(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "system_prompt_rewrite_query", value)

    @_builtins.property
    @pulumi.getter(name="tokenId")
    def token_id(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "token_id")

    @token_id.setter
    def token_id(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "token_id", value)


@pulumi.input_type
class _AiSearchInstanceState:
    def __init__(__self__, *,
                 account_id: Optional[pulumi.Input[_builtins.str]] = None,
                 account_tag: Optional[pulumi.Input[_builtins.str]] = None,
                 ai_gateway_id: Optional[pulumi.Input[_builtins.str]] = None,
                 ai_search_id: Optional[pulumi.Input[_builtins.str]] = None,
                 aisearch_model: Optional[pulumi.Input[_builtins.str]] = None,
                 cache: Optional[pulumi.Input[_builtins.bool]] = None,
                 cache_threshold: Optional[pulumi.Input[_builtins.str]] = None,
                 chunk: Optional[pulumi.Input[_builtins.bool]] = None,
                 chunk_overlap: Optional[pulumi.Input[_builtins.int]] = None,
                 chunk_size: Optional[pulumi.Input[_builtins.int]] = None,
                 created_at: Optional[pulumi.Input[_builtins.str]] = None,
                 created_by: Optional[pulumi.Input[_builtins.str]] = None,
                 custom_metadatas: Optional[pulumi.Input[Sequence[pulumi.Input['AiSearchInstanceCustomMetadataArgs']]]] = None,
                 embedding_model: Optional[pulumi.Input[_builtins.str]] = None,
                 enable: Optional[pulumi.Input[_builtins.bool]] = None,
                 engine_version: Optional[pulumi.Input[_builtins.float]] = None,
                 hybrid_search_enabled: Optional[pulumi.Input[_builtins.bool]] = None,
                 internal_id: Optional[pulumi.Input[_builtins.str]] = None,
                 last_activity: Optional[pulumi.Input[_builtins.str]] = None,
                 max_num_results: Optional[pulumi.Input[_builtins.int]] = None,
                 metadata: Optional[pulumi.Input['AiSearchInstanceMetadataArgs']] = None,
                 modified_at: Optional[pulumi.Input[_builtins.str]] = None,
                 modified_by: Optional[pulumi.Input[_builtins.str]] = None,
                 paused: Optional[pulumi.Input[_builtins.bool]] = None,
                 public_endpoint_id: Optional[pulumi.Input[_builtins.str]] = None,
                 public_endpoint_params: Optional[pulumi.Input['AiSearchInstancePublicEndpointParamsArgs']] = None,
                 reranking: Optional[pulumi.Input[_builtins.bool]] = None,
                 reranking_model: Optional[pulumi.Input[_builtins.str]] = None,
                 rewrite_model: Optional[pulumi.Input[_builtins.str]] = None,
                 rewrite_query: Optional[pulumi.Input[_builtins.bool]] = None,
                 score_threshold: Optional[pulumi.Input[_builtins.float]] = None,
                 source: Optional[pulumi.Input[_builtins.str]] = None,
                 source_params: Optional[pulumi.Input['AiSearchInstanceSourceParamsArgs']] = None,
                 status: Optional[pulumi.Input[_builtins.str]] = None,
                 summarization: Optional[pulumi.Input[_builtins.bool]] = None,
                 summarization_model: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_aisearch: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_index_summarization: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_rewrite_query: Optional[pulumi.Input[_builtins.str]] = None,
                 token_id: Optional[pulumi.Input[_builtins.str]] = None,
                 type: Optional[pulumi.Input[_builtins.str]] = None,
                 vectorize_active_namespace: Optional[pulumi.Input[_builtins.str]] = None,
                 vectorize_name: Optional[pulumi.Input[_builtins.str]] = None):
        """
        Input properties used for looking up and filtering AiSearchInstance resources.
        :param pulumi.Input[_builtins.str] ai_search_id: Use your AI Search ID.
        :param pulumi.Input[_builtins.str] aisearch_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] cache_threshold: Available values: "super*strict*match", "close*enough", "flexible*friend", "anything_goes".
        :param pulumi.Input[_builtins.str] embedding_model: Available values: "@cf/qwen/qwen3-embedding-0.6b", "@cf/baai/bge-m3", "@cf/baai/bge-large-en-v1.5", "@cf/google/embeddinggemma-300m", "google-ai-studio/gemini-embedding-001", "openai/text-embedding-3-small", "openai/text-embedding-3-large", "".
        :param pulumi.Input[_builtins.str] reranking_model: Available values: "@cf/baai/bge-reranker-base", "".
        :param pulumi.Input[_builtins.str] rewrite_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] summarization_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] type: Available values: "r2", "web-crawler".
        """
        if account_id is not None:
            pulumi.set(__self__, "account_id", account_id)
        if account_tag is not None:
            pulumi.set(__self__, "account_tag", account_tag)
        if ai_gateway_id is not None:
            pulumi.set(__self__, "ai_gateway_id", ai_gateway_id)
        if ai_search_id is not None:
            pulumi.set(__self__, "ai_search_id", ai_search_id)
        if aisearch_model is not None:
            pulumi.set(__self__, "aisearch_model", aisearch_model)
        if cache is not None:
            pulumi.set(__self__, "cache", cache)
        if cache_threshold is not None:
            pulumi.set(__self__, "cache_threshold", cache_threshold)
        if chunk is not None:
            pulumi.set(__self__, "chunk", chunk)
        if chunk_overlap is not None:
            pulumi.set(__self__, "chunk_overlap", chunk_overlap)
        if chunk_size is not None:
            pulumi.set(__self__, "chunk_size", chunk_size)
        if created_at is not None:
            pulumi.set(__self__, "created_at", created_at)
        if created_by is not None:
            pulumi.set(__self__, "created_by", created_by)
        if custom_metadatas is not None:
            pulumi.set(__self__, "custom_metadatas", custom_metadatas)
        if embedding_model is not None:
            pulumi.set(__self__, "embedding_model", embedding_model)
        if enable is not None:
            pulumi.set(__self__, "enable", enable)
        if engine_version is not None:
            pulumi.set(__self__, "engine_version", engine_version)
        if hybrid_search_enabled is not None:
            pulumi.set(__self__, "hybrid_search_enabled", hybrid_search_enabled)
        if internal_id is not None:
            pulumi.set(__self__, "internal_id", internal_id)
        if last_activity is not None:
            pulumi.set(__self__, "last_activity", last_activity)
        if max_num_results is not None:
            pulumi.set(__self__, "max_num_results", max_num_results)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)
        if modified_at is not None:
            pulumi.set(__self__, "modified_at", modified_at)
        if modified_by is not None:
            pulumi.set(__self__, "modified_by", modified_by)
        if paused is not None:
            pulumi.set(__self__, "paused", paused)
        if public_endpoint_id is not None:
            pulumi.set(__self__, "public_endpoint_id", public_endpoint_id)
        if public_endpoint_params is not None:
            pulumi.set(__self__, "public_endpoint_params", public_endpoint_params)
        if reranking is not None:
            pulumi.set(__self__, "reranking", reranking)
        if reranking_model is not None:
            pulumi.set(__self__, "reranking_model", reranking_model)
        if rewrite_model is not None:
            pulumi.set(__self__, "rewrite_model", rewrite_model)
        if rewrite_query is not None:
            pulumi.set(__self__, "rewrite_query", rewrite_query)
        if score_threshold is not None:
            pulumi.set(__self__, "score_threshold", score_threshold)
        if source is not None:
            pulumi.set(__self__, "source", source)
        if source_params is not None:
            pulumi.set(__self__, "source_params", source_params)
        if status is not None:
            pulumi.set(__self__, "status", status)
        if summarization is not None:
            pulumi.set(__self__, "summarization", summarization)
        if summarization_model is not None:
            pulumi.set(__self__, "summarization_model", summarization_model)
        if system_prompt_aisearch is not None:
            pulumi.set(__self__, "system_prompt_aisearch", system_prompt_aisearch)
        if system_prompt_index_summarization is not None:
            pulumi.set(__self__, "system_prompt_index_summarization", system_prompt_index_summarization)
        if system_prompt_rewrite_query is not None:
            pulumi.set(__self__, "system_prompt_rewrite_query", system_prompt_rewrite_query)
        if token_id is not None:
            pulumi.set(__self__, "token_id", token_id)
        if type is not None:
            pulumi.set(__self__, "type", type)
        if vectorize_active_namespace is not None:
            pulumi.set(__self__, "vectorize_active_namespace", vectorize_active_namespace)
        if vectorize_name is not None:
            pulumi.set(__self__, "vectorize_name", vectorize_name)

    @_builtins.property
    @pulumi.getter(name="accountId")
    def account_id(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "account_id")

    @account_id.setter
    def account_id(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "account_id", value)

    @_builtins.property
    @pulumi.getter(name="accountTag")
    def account_tag(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "account_tag")

    @account_tag.setter
    def account_tag(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "account_tag", value)

    @_builtins.property
    @pulumi.getter(name="aiGatewayId")
    def ai_gateway_id(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "ai_gateway_id")

    @ai_gateway_id.setter
    def ai_gateway_id(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "ai_gateway_id", value)

    @_builtins.property
    @pulumi.getter(name="aiSearchId")
    def ai_search_id(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Use your AI Search ID.
        """
        return pulumi.get(self, "ai_search_id")

    @ai_search_id.setter
    def ai_search_id(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "ai_search_id", value)

    @_builtins.property
    @pulumi.getter(name="aisearchModel")
    def aisearch_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        return pulumi.get(self, "aisearch_model")

    @aisearch_model.setter
    def aisearch_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "aisearch_model", value)

    @_builtins.property
    @pulumi.getter
    def cache(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "cache")

    @cache.setter
    def cache(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "cache", value)

    @_builtins.property
    @pulumi.getter(name="cacheThreshold")
    def cache_threshold(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "super*strict*match", "close*enough", "flexible*friend", "anything_goes".
        """
        return pulumi.get(self, "cache_threshold")

    @cache_threshold.setter
    def cache_threshold(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "cache_threshold", value)

    @_builtins.property
    @pulumi.getter
    def chunk(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "chunk")

    @chunk.setter
    def chunk(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "chunk", value)

    @_builtins.property
    @pulumi.getter(name="chunkOverlap")
    def chunk_overlap(self) -> Optional[pulumi.Input[_builtins.int]]:
        return pulumi.get(self, "chunk_overlap")

    @chunk_overlap.setter
    def chunk_overlap(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "chunk_overlap", value)

    @_builtins.property
    @pulumi.getter(name="chunkSize")
    def chunk_size(self) -> Optional[pulumi.Input[_builtins.int]]:
        return pulumi.get(self, "chunk_size")

    @chunk_size.setter
    def chunk_size(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "chunk_size", value)

    @_builtins.property
    @pulumi.getter(name="createdAt")
    def created_at(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "created_at")

    @created_at.setter
    def created_at(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "created_at", value)

    @_builtins.property
    @pulumi.getter(name="createdBy")
    def created_by(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "created_by")

    @created_by.setter
    def created_by(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "created_by", value)

    @_builtins.property
    @pulumi.getter(name="customMetadatas")
    def custom_metadatas(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['AiSearchInstanceCustomMetadataArgs']]]]:
        return pulumi.get(self, "custom_metadatas")

    @custom_metadatas.setter
    def custom_metadatas(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['AiSearchInstanceCustomMetadataArgs']]]]):
        pulumi.set(self, "custom_metadatas", value)

    @_builtins.property
    @pulumi.getter(name="embeddingModel")
    def embedding_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/qwen/qwen3-embedding-0.6b", "@cf/baai/bge-m3", "@cf/baai/bge-large-en-v1.5", "@cf/google/embeddinggemma-300m", "google-ai-studio/gemini-embedding-001", "openai/text-embedding-3-small", "openai/text-embedding-3-large", "".
        """
        return pulumi.get(self, "embedding_model")

    @embedding_model.setter
    def embedding_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "embedding_model", value)

    @_builtins.property
    @pulumi.getter
    def enable(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "enable")

    @enable.setter
    def enable(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "enable", value)

    @_builtins.property
    @pulumi.getter(name="engineVersion")
    def engine_version(self) -> Optional[pulumi.Input[_builtins.float]]:
        return pulumi.get(self, "engine_version")

    @engine_version.setter
    def engine_version(self, value: Optional[pulumi.Input[_builtins.float]]):
        pulumi.set(self, "engine_version", value)

    @_builtins.property
    @pulumi.getter(name="hybridSearchEnabled")
    def hybrid_search_enabled(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "hybrid_search_enabled")

    @hybrid_search_enabled.setter
    def hybrid_search_enabled(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "hybrid_search_enabled", value)

    @_builtins.property
    @pulumi.getter(name="internalId")
    def internal_id(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "internal_id")

    @internal_id.setter
    def internal_id(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "internal_id", value)

    @_builtins.property
    @pulumi.getter(name="lastActivity")
    def last_activity(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "last_activity")

    @last_activity.setter
    def last_activity(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "last_activity", value)

    @_builtins.property
    @pulumi.getter(name="maxNumResults")
    def max_num_results(self) -> Optional[pulumi.Input[_builtins.int]]:
        return pulumi.get(self, "max_num_results")

    @max_num_results.setter
    def max_num_results(self, value: Optional[pulumi.Input[_builtins.int]]):
        pulumi.set(self, "max_num_results", value)

    @_builtins.property
    @pulumi.getter
    def metadata(self) -> Optional[pulumi.Input['AiSearchInstanceMetadataArgs']]:
        return pulumi.get(self, "metadata")

    @metadata.setter
    def metadata(self, value: Optional[pulumi.Input['AiSearchInstanceMetadataArgs']]):
        pulumi.set(self, "metadata", value)

    @_builtins.property
    @pulumi.getter(name="modifiedAt")
    def modified_at(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "modified_at")

    @modified_at.setter
    def modified_at(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "modified_at", value)

    @_builtins.property
    @pulumi.getter(name="modifiedBy")
    def modified_by(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "modified_by")

    @modified_by.setter
    def modified_by(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "modified_by", value)

    @_builtins.property
    @pulumi.getter
    def paused(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "paused")

    @paused.setter
    def paused(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "paused", value)

    @_builtins.property
    @pulumi.getter(name="publicEndpointId")
    def public_endpoint_id(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "public_endpoint_id")

    @public_endpoint_id.setter
    def public_endpoint_id(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "public_endpoint_id", value)

    @_builtins.property
    @pulumi.getter(name="publicEndpointParams")
    def public_endpoint_params(self) -> Optional[pulumi.Input['AiSearchInstancePublicEndpointParamsArgs']]:
        return pulumi.get(self, "public_endpoint_params")

    @public_endpoint_params.setter
    def public_endpoint_params(self, value: Optional[pulumi.Input['AiSearchInstancePublicEndpointParamsArgs']]):
        pulumi.set(self, "public_endpoint_params", value)

    @_builtins.property
    @pulumi.getter
    def reranking(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "reranking")

    @reranking.setter
    def reranking(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "reranking", value)

    @_builtins.property
    @pulumi.getter(name="rerankingModel")
    def reranking_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/baai/bge-reranker-base", "".
        """
        return pulumi.get(self, "reranking_model")

    @reranking_model.setter
    def reranking_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "reranking_model", value)

    @_builtins.property
    @pulumi.getter(name="rewriteModel")
    def rewrite_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        return pulumi.get(self, "rewrite_model")

    @rewrite_model.setter
    def rewrite_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "rewrite_model", value)

    @_builtins.property
    @pulumi.getter(name="rewriteQuery")
    def rewrite_query(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "rewrite_query")

    @rewrite_query.setter
    def rewrite_query(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "rewrite_query", value)

    @_builtins.property
    @pulumi.getter(name="scoreThreshold")
    def score_threshold(self) -> Optional[pulumi.Input[_builtins.float]]:
        return pulumi.get(self, "score_threshold")

    @score_threshold.setter
    def score_threshold(self, value: Optional[pulumi.Input[_builtins.float]]):
        pulumi.set(self, "score_threshold", value)

    @_builtins.property
    @pulumi.getter
    def source(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "source")

    @source.setter
    def source(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "source", value)

    @_builtins.property
    @pulumi.getter(name="sourceParams")
    def source_params(self) -> Optional[pulumi.Input['AiSearchInstanceSourceParamsArgs']]:
        return pulumi.get(self, "source_params")

    @source_params.setter
    def source_params(self, value: Optional[pulumi.Input['AiSearchInstanceSourceParamsArgs']]):
        pulumi.set(self, "source_params", value)

    @_builtins.property
    @pulumi.getter
    def status(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "status")

    @status.setter
    def status(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "status", value)

    @_builtins.property
    @pulumi.getter
    def summarization(self) -> Optional[pulumi.Input[_builtins.bool]]:
        return pulumi.get(self, "summarization")

    @summarization.setter
    def summarization(self, value: Optional[pulumi.Input[_builtins.bool]]):
        pulumi.set(self, "summarization", value)

    @_builtins.property
    @pulumi.getter(name="summarizationModel")
    def summarization_model(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        return pulumi.get(self, "summarization_model")

    @summarization_model.setter
    def summarization_model(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "summarization_model", value)

    @_builtins.property
    @pulumi.getter(name="systemPromptAisearch")
    def system_prompt_aisearch(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "system_prompt_aisearch")

    @system_prompt_aisearch.setter
    def system_prompt_aisearch(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "system_prompt_aisearch", value)

    @_builtins.property
    @pulumi.getter(name="systemPromptIndexSummarization")
    def system_prompt_index_summarization(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "system_prompt_index_summarization")

    @system_prompt_index_summarization.setter
    def system_prompt_index_summarization(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "system_prompt_index_summarization", value)

    @_builtins.property
    @pulumi.getter(name="systemPromptRewriteQuery")
    def system_prompt_rewrite_query(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "system_prompt_rewrite_query")

    @system_prompt_rewrite_query.setter
    def system_prompt_rewrite_query(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "system_prompt_rewrite_query", value)

    @_builtins.property
    @pulumi.getter(name="tokenId")
    def token_id(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "token_id")

    @token_id.setter
    def token_id(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "token_id", value)

    @_builtins.property
    @pulumi.getter
    def type(self) -> Optional[pulumi.Input[_builtins.str]]:
        """
        Available values: "r2", "web-crawler".
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "type", value)

    @_builtins.property
    @pulumi.getter(name="vectorizeActiveNamespace")
    def vectorize_active_namespace(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "vectorize_active_namespace")

    @vectorize_active_namespace.setter
    def vectorize_active_namespace(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "vectorize_active_namespace", value)

    @_builtins.property
    @pulumi.getter(name="vectorizeName")
    def vectorize_name(self) -> Optional[pulumi.Input[_builtins.str]]:
        return pulumi.get(self, "vectorize_name")

    @vectorize_name.setter
    def vectorize_name(self, value: Optional[pulumi.Input[_builtins.str]]):
        pulumi.set(self, "vectorize_name", value)


@pulumi.type_token("cloudflare:index/aiSearchInstance:AiSearchInstance")
class AiSearchInstance(pulumi.CustomResource):
    @overload
    def __init__(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 account_id: Optional[pulumi.Input[_builtins.str]] = None,
                 ai_gateway_id: Optional[pulumi.Input[_builtins.str]] = None,
                 ai_search_id: Optional[pulumi.Input[_builtins.str]] = None,
                 aisearch_model: Optional[pulumi.Input[_builtins.str]] = None,
                 cache: Optional[pulumi.Input[_builtins.bool]] = None,
                 cache_threshold: Optional[pulumi.Input[_builtins.str]] = None,
                 chunk: Optional[pulumi.Input[_builtins.bool]] = None,
                 chunk_overlap: Optional[pulumi.Input[_builtins.int]] = None,
                 chunk_size: Optional[pulumi.Input[_builtins.int]] = None,
                 custom_metadatas: Optional[pulumi.Input[Sequence[pulumi.Input[Union['AiSearchInstanceCustomMetadataArgs', 'AiSearchInstanceCustomMetadataArgsDict']]]]] = None,
                 embedding_model: Optional[pulumi.Input[_builtins.str]] = None,
                 hybrid_search_enabled: Optional[pulumi.Input[_builtins.bool]] = None,
                 max_num_results: Optional[pulumi.Input[_builtins.int]] = None,
                 metadata: Optional[pulumi.Input[Union['AiSearchInstanceMetadataArgs', 'AiSearchInstanceMetadataArgsDict']]] = None,
                 paused: Optional[pulumi.Input[_builtins.bool]] = None,
                 public_endpoint_params: Optional[pulumi.Input[Union['AiSearchInstancePublicEndpointParamsArgs', 'AiSearchInstancePublicEndpointParamsArgsDict']]] = None,
                 reranking: Optional[pulumi.Input[_builtins.bool]] = None,
                 reranking_model: Optional[pulumi.Input[_builtins.str]] = None,
                 rewrite_model: Optional[pulumi.Input[_builtins.str]] = None,
                 rewrite_query: Optional[pulumi.Input[_builtins.bool]] = None,
                 score_threshold: Optional[pulumi.Input[_builtins.float]] = None,
                 source: Optional[pulumi.Input[_builtins.str]] = None,
                 source_params: Optional[pulumi.Input[Union['AiSearchInstanceSourceParamsArgs', 'AiSearchInstanceSourceParamsArgsDict']]] = None,
                 summarization: Optional[pulumi.Input[_builtins.bool]] = None,
                 summarization_model: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_aisearch: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_index_summarization: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_rewrite_query: Optional[pulumi.Input[_builtins.str]] = None,
                 token_id: Optional[pulumi.Input[_builtins.str]] = None,
                 type: Optional[pulumi.Input[_builtins.str]] = None,
                 __props__=None):
        """
        ## Import

        > This resource does not currently support `pulumi import`.

        :param str resource_name: The name of the resource.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[_builtins.str] ai_search_id: Use your AI Search ID.
        :param pulumi.Input[_builtins.str] aisearch_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] cache_threshold: Available values: "super*strict*match", "close*enough", "flexible*friend", "anything_goes".
        :param pulumi.Input[_builtins.str] embedding_model: Available values: "@cf/qwen/qwen3-embedding-0.6b", "@cf/baai/bge-m3", "@cf/baai/bge-large-en-v1.5", "@cf/google/embeddinggemma-300m", "google-ai-studio/gemini-embedding-001", "openai/text-embedding-3-small", "openai/text-embedding-3-large", "".
        :param pulumi.Input[_builtins.str] reranking_model: Available values: "@cf/baai/bge-reranker-base", "".
        :param pulumi.Input[_builtins.str] rewrite_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] summarization_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] type: Available values: "r2", "web-crawler".
        """
        ...
    @overload
    def __init__(__self__,
                 resource_name: str,
                 args: AiSearchInstanceArgs,
                 opts: Optional[pulumi.ResourceOptions] = None):
        """
        ## Import

        > This resource does not currently support `pulumi import`.

        :param str resource_name: The name of the resource.
        :param AiSearchInstanceArgs args: The arguments to use to populate this resource's properties.
        :param pulumi.ResourceOptions opts: Options for the resource.
        """
        ...
    def __init__(__self__, resource_name: str, *args, **kwargs):
        resource_args, opts = _utilities.get_resource_args_opts(AiSearchInstanceArgs, pulumi.ResourceOptions, *args, **kwargs)
        if resource_args is not None:
            __self__._internal_init(resource_name, opts, **resource_args.__dict__)
        else:
            __self__._internal_init(resource_name, *args, **kwargs)

    def _internal_init(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 account_id: Optional[pulumi.Input[_builtins.str]] = None,
                 ai_gateway_id: Optional[pulumi.Input[_builtins.str]] = None,
                 ai_search_id: Optional[pulumi.Input[_builtins.str]] = None,
                 aisearch_model: Optional[pulumi.Input[_builtins.str]] = None,
                 cache: Optional[pulumi.Input[_builtins.bool]] = None,
                 cache_threshold: Optional[pulumi.Input[_builtins.str]] = None,
                 chunk: Optional[pulumi.Input[_builtins.bool]] = None,
                 chunk_overlap: Optional[pulumi.Input[_builtins.int]] = None,
                 chunk_size: Optional[pulumi.Input[_builtins.int]] = None,
                 custom_metadatas: Optional[pulumi.Input[Sequence[pulumi.Input[Union['AiSearchInstanceCustomMetadataArgs', 'AiSearchInstanceCustomMetadataArgsDict']]]]] = None,
                 embedding_model: Optional[pulumi.Input[_builtins.str]] = None,
                 hybrid_search_enabled: Optional[pulumi.Input[_builtins.bool]] = None,
                 max_num_results: Optional[pulumi.Input[_builtins.int]] = None,
                 metadata: Optional[pulumi.Input[Union['AiSearchInstanceMetadataArgs', 'AiSearchInstanceMetadataArgsDict']]] = None,
                 paused: Optional[pulumi.Input[_builtins.bool]] = None,
                 public_endpoint_params: Optional[pulumi.Input[Union['AiSearchInstancePublicEndpointParamsArgs', 'AiSearchInstancePublicEndpointParamsArgsDict']]] = None,
                 reranking: Optional[pulumi.Input[_builtins.bool]] = None,
                 reranking_model: Optional[pulumi.Input[_builtins.str]] = None,
                 rewrite_model: Optional[pulumi.Input[_builtins.str]] = None,
                 rewrite_query: Optional[pulumi.Input[_builtins.bool]] = None,
                 score_threshold: Optional[pulumi.Input[_builtins.float]] = None,
                 source: Optional[pulumi.Input[_builtins.str]] = None,
                 source_params: Optional[pulumi.Input[Union['AiSearchInstanceSourceParamsArgs', 'AiSearchInstanceSourceParamsArgsDict']]] = None,
                 summarization: Optional[pulumi.Input[_builtins.bool]] = None,
                 summarization_model: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_aisearch: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_index_summarization: Optional[pulumi.Input[_builtins.str]] = None,
                 system_prompt_rewrite_query: Optional[pulumi.Input[_builtins.str]] = None,
                 token_id: Optional[pulumi.Input[_builtins.str]] = None,
                 type: Optional[pulumi.Input[_builtins.str]] = None,
                 __props__=None):
        opts = pulumi.ResourceOptions.merge(_utilities.get_resource_opts_defaults(), opts)
        if not isinstance(opts, pulumi.ResourceOptions):
            raise TypeError('Expected resource options to be a ResourceOptions instance')
        if opts.id is None:
            if __props__ is not None:
                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')
            __props__ = AiSearchInstanceArgs.__new__(AiSearchInstanceArgs)

            if account_id is None and not opts.urn:
                raise TypeError("Missing required property 'account_id'")
            __props__.__dict__["account_id"] = account_id
            __props__.__dict__["ai_gateway_id"] = ai_gateway_id
            if ai_search_id is None and not opts.urn:
                raise TypeError("Missing required property 'ai_search_id'")
            __props__.__dict__["ai_search_id"] = ai_search_id
            __props__.__dict__["aisearch_model"] = aisearch_model
            __props__.__dict__["cache"] = cache
            __props__.__dict__["cache_threshold"] = cache_threshold
            __props__.__dict__["chunk"] = chunk
            __props__.__dict__["chunk_overlap"] = chunk_overlap
            __props__.__dict__["chunk_size"] = chunk_size
            __props__.__dict__["custom_metadatas"] = custom_metadatas
            __props__.__dict__["embedding_model"] = embedding_model
            __props__.__dict__["hybrid_search_enabled"] = hybrid_search_enabled
            __props__.__dict__["max_num_results"] = max_num_results
            __props__.__dict__["metadata"] = metadata
            __props__.__dict__["paused"] = paused
            __props__.__dict__["public_endpoint_params"] = public_endpoint_params
            __props__.__dict__["reranking"] = reranking
            __props__.__dict__["reranking_model"] = reranking_model
            __props__.__dict__["rewrite_model"] = rewrite_model
            __props__.__dict__["rewrite_query"] = rewrite_query
            __props__.__dict__["score_threshold"] = score_threshold
            if source is None and not opts.urn:
                raise TypeError("Missing required property 'source'")
            __props__.__dict__["source"] = source
            __props__.__dict__["source_params"] = source_params
            __props__.__dict__["summarization"] = summarization
            __props__.__dict__["summarization_model"] = summarization_model
            __props__.__dict__["system_prompt_aisearch"] = system_prompt_aisearch
            __props__.__dict__["system_prompt_index_summarization"] = system_prompt_index_summarization
            __props__.__dict__["system_prompt_rewrite_query"] = system_prompt_rewrite_query
            __props__.__dict__["token_id"] = token_id
            if type is None and not opts.urn:
                raise TypeError("Missing required property 'type'")
            __props__.__dict__["type"] = type
            __props__.__dict__["account_tag"] = None
            __props__.__dict__["created_at"] = None
            __props__.__dict__["created_by"] = None
            __props__.__dict__["enable"] = None
            __props__.__dict__["engine_version"] = None
            __props__.__dict__["internal_id"] = None
            __props__.__dict__["last_activity"] = None
            __props__.__dict__["modified_at"] = None
            __props__.__dict__["modified_by"] = None
            __props__.__dict__["public_endpoint_id"] = None
            __props__.__dict__["status"] = None
            __props__.__dict__["vectorize_active_namespace"] = None
            __props__.__dict__["vectorize_name"] = None
        super(AiSearchInstance, __self__).__init__(
            'cloudflare:index/aiSearchInstance:AiSearchInstance',
            resource_name,
            __props__,
            opts)

    @staticmethod
    def get(resource_name: str,
            id: pulumi.Input[str],
            opts: Optional[pulumi.ResourceOptions] = None,
            account_id: Optional[pulumi.Input[_builtins.str]] = None,
            account_tag: Optional[pulumi.Input[_builtins.str]] = None,
            ai_gateway_id: Optional[pulumi.Input[_builtins.str]] = None,
            ai_search_id: Optional[pulumi.Input[_builtins.str]] = None,
            aisearch_model: Optional[pulumi.Input[_builtins.str]] = None,
            cache: Optional[pulumi.Input[_builtins.bool]] = None,
            cache_threshold: Optional[pulumi.Input[_builtins.str]] = None,
            chunk: Optional[pulumi.Input[_builtins.bool]] = None,
            chunk_overlap: Optional[pulumi.Input[_builtins.int]] = None,
            chunk_size: Optional[pulumi.Input[_builtins.int]] = None,
            created_at: Optional[pulumi.Input[_builtins.str]] = None,
            created_by: Optional[pulumi.Input[_builtins.str]] = None,
            custom_metadatas: Optional[pulumi.Input[Sequence[pulumi.Input[Union['AiSearchInstanceCustomMetadataArgs', 'AiSearchInstanceCustomMetadataArgsDict']]]]] = None,
            embedding_model: Optional[pulumi.Input[_builtins.str]] = None,
            enable: Optional[pulumi.Input[_builtins.bool]] = None,
            engine_version: Optional[pulumi.Input[_builtins.float]] = None,
            hybrid_search_enabled: Optional[pulumi.Input[_builtins.bool]] = None,
            internal_id: Optional[pulumi.Input[_builtins.str]] = None,
            last_activity: Optional[pulumi.Input[_builtins.str]] = None,
            max_num_results: Optional[pulumi.Input[_builtins.int]] = None,
            metadata: Optional[pulumi.Input[Union['AiSearchInstanceMetadataArgs', 'AiSearchInstanceMetadataArgsDict']]] = None,
            modified_at: Optional[pulumi.Input[_builtins.str]] = None,
            modified_by: Optional[pulumi.Input[_builtins.str]] = None,
            paused: Optional[pulumi.Input[_builtins.bool]] = None,
            public_endpoint_id: Optional[pulumi.Input[_builtins.str]] = None,
            public_endpoint_params: Optional[pulumi.Input[Union['AiSearchInstancePublicEndpointParamsArgs', 'AiSearchInstancePublicEndpointParamsArgsDict']]] = None,
            reranking: Optional[pulumi.Input[_builtins.bool]] = None,
            reranking_model: Optional[pulumi.Input[_builtins.str]] = None,
            rewrite_model: Optional[pulumi.Input[_builtins.str]] = None,
            rewrite_query: Optional[pulumi.Input[_builtins.bool]] = None,
            score_threshold: Optional[pulumi.Input[_builtins.float]] = None,
            source: Optional[pulumi.Input[_builtins.str]] = None,
            source_params: Optional[pulumi.Input[Union['AiSearchInstanceSourceParamsArgs', 'AiSearchInstanceSourceParamsArgsDict']]] = None,
            status: Optional[pulumi.Input[_builtins.str]] = None,
            summarization: Optional[pulumi.Input[_builtins.bool]] = None,
            summarization_model: Optional[pulumi.Input[_builtins.str]] = None,
            system_prompt_aisearch: Optional[pulumi.Input[_builtins.str]] = None,
            system_prompt_index_summarization: Optional[pulumi.Input[_builtins.str]] = None,
            system_prompt_rewrite_query: Optional[pulumi.Input[_builtins.str]] = None,
            token_id: Optional[pulumi.Input[_builtins.str]] = None,
            type: Optional[pulumi.Input[_builtins.str]] = None,
            vectorize_active_namespace: Optional[pulumi.Input[_builtins.str]] = None,
            vectorize_name: Optional[pulumi.Input[_builtins.str]] = None) -> 'AiSearchInstance':
        """
        Get an existing AiSearchInstance resource's state with the given name, id, and optional extra
        properties used to qualify the lookup.

        :param str resource_name: The unique name of the resulting resource.
        :param pulumi.Input[str] id: The unique provider ID of the resource to lookup.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[_builtins.str] ai_search_id: Use your AI Search ID.
        :param pulumi.Input[_builtins.str] aisearch_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] cache_threshold: Available values: "super*strict*match", "close*enough", "flexible*friend", "anything_goes".
        :param pulumi.Input[_builtins.str] embedding_model: Available values: "@cf/qwen/qwen3-embedding-0.6b", "@cf/baai/bge-m3", "@cf/baai/bge-large-en-v1.5", "@cf/google/embeddinggemma-300m", "google-ai-studio/gemini-embedding-001", "openai/text-embedding-3-small", "openai/text-embedding-3-large", "".
        :param pulumi.Input[_builtins.str] reranking_model: Available values: "@cf/baai/bge-reranker-base", "".
        :param pulumi.Input[_builtins.str] rewrite_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] summarization_model: Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        :param pulumi.Input[_builtins.str] type: Available values: "r2", "web-crawler".
        """
        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))

        __props__ = _AiSearchInstanceState.__new__(_AiSearchInstanceState)

        __props__.__dict__["account_id"] = account_id
        __props__.__dict__["account_tag"] = account_tag
        __props__.__dict__["ai_gateway_id"] = ai_gateway_id
        __props__.__dict__["ai_search_id"] = ai_search_id
        __props__.__dict__["aisearch_model"] = aisearch_model
        __props__.__dict__["cache"] = cache
        __props__.__dict__["cache_threshold"] = cache_threshold
        __props__.__dict__["chunk"] = chunk
        __props__.__dict__["chunk_overlap"] = chunk_overlap
        __props__.__dict__["chunk_size"] = chunk_size
        __props__.__dict__["created_at"] = created_at
        __props__.__dict__["created_by"] = created_by
        __props__.__dict__["custom_metadatas"] = custom_metadatas
        __props__.__dict__["embedding_model"] = embedding_model
        __props__.__dict__["enable"] = enable
        __props__.__dict__["engine_version"] = engine_version
        __props__.__dict__["hybrid_search_enabled"] = hybrid_search_enabled
        __props__.__dict__["internal_id"] = internal_id
        __props__.__dict__["last_activity"] = last_activity
        __props__.__dict__["max_num_results"] = max_num_results
        __props__.__dict__["metadata"] = metadata
        __props__.__dict__["modified_at"] = modified_at
        __props__.__dict__["modified_by"] = modified_by
        __props__.__dict__["paused"] = paused
        __props__.__dict__["public_endpoint_id"] = public_endpoint_id
        __props__.__dict__["public_endpoint_params"] = public_endpoint_params
        __props__.__dict__["reranking"] = reranking
        __props__.__dict__["reranking_model"] = reranking_model
        __props__.__dict__["rewrite_model"] = rewrite_model
        __props__.__dict__["rewrite_query"] = rewrite_query
        __props__.__dict__["score_threshold"] = score_threshold
        __props__.__dict__["source"] = source
        __props__.__dict__["source_params"] = source_params
        __props__.__dict__["status"] = status
        __props__.__dict__["summarization"] = summarization
        __props__.__dict__["summarization_model"] = summarization_model
        __props__.__dict__["system_prompt_aisearch"] = system_prompt_aisearch
        __props__.__dict__["system_prompt_index_summarization"] = system_prompt_index_summarization
        __props__.__dict__["system_prompt_rewrite_query"] = system_prompt_rewrite_query
        __props__.__dict__["token_id"] = token_id
        __props__.__dict__["type"] = type
        __props__.__dict__["vectorize_active_namespace"] = vectorize_active_namespace
        __props__.__dict__["vectorize_name"] = vectorize_name
        return AiSearchInstance(resource_name, opts=opts, __props__=__props__)

    @_builtins.property
    @pulumi.getter(name="accountId")
    def account_id(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "account_id")

    @_builtins.property
    @pulumi.getter(name="accountTag")
    def account_tag(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "account_tag")

    @_builtins.property
    @pulumi.getter(name="aiGatewayId")
    def ai_gateway_id(self) -> pulumi.Output[Optional[_builtins.str]]:
        return pulumi.get(self, "ai_gateway_id")

    @_builtins.property
    @pulumi.getter(name="aiSearchId")
    def ai_search_id(self) -> pulumi.Output[_builtins.str]:
        """
        Use your AI Search ID.
        """
        return pulumi.get(self, "ai_search_id")

    @_builtins.property
    @pulumi.getter(name="aisearchModel")
    def aisearch_model(self) -> pulumi.Output[Optional[_builtins.str]]:
        """
        Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        return pulumi.get(self, "aisearch_model")

    @_builtins.property
    @pulumi.getter
    def cache(self) -> pulumi.Output[_builtins.bool]:
        return pulumi.get(self, "cache")

    @_builtins.property
    @pulumi.getter(name="cacheThreshold")
    def cache_threshold(self) -> pulumi.Output[_builtins.str]:
        """
        Available values: "super*strict*match", "close*enough", "flexible*friend", "anything_goes".
        """
        return pulumi.get(self, "cache_threshold")

    @_builtins.property
    @pulumi.getter
    def chunk(self) -> pulumi.Output[_builtins.bool]:
        return pulumi.get(self, "chunk")

    @_builtins.property
    @pulumi.getter(name="chunkOverlap")
    def chunk_overlap(self) -> pulumi.Output[_builtins.int]:
        return pulumi.get(self, "chunk_overlap")

    @_builtins.property
    @pulumi.getter(name="chunkSize")
    def chunk_size(self) -> pulumi.Output[_builtins.int]:
        return pulumi.get(self, "chunk_size")

    @_builtins.property
    @pulumi.getter(name="createdAt")
    def created_at(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "created_at")

    @_builtins.property
    @pulumi.getter(name="createdBy")
    def created_by(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "created_by")

    @_builtins.property
    @pulumi.getter(name="customMetadatas")
    def custom_metadatas(self) -> pulumi.Output[Optional[Sequence['outputs.AiSearchInstanceCustomMetadata']]]:
        return pulumi.get(self, "custom_metadatas")

    @_builtins.property
    @pulumi.getter(name="embeddingModel")
    def embedding_model(self) -> pulumi.Output[Optional[_builtins.str]]:
        """
        Available values: "@cf/qwen/qwen3-embedding-0.6b", "@cf/baai/bge-m3", "@cf/baai/bge-large-en-v1.5", "@cf/google/embeddinggemma-300m", "google-ai-studio/gemini-embedding-001", "openai/text-embedding-3-small", "openai/text-embedding-3-large", "".
        """
        return pulumi.get(self, "embedding_model")

    @_builtins.property
    @pulumi.getter
    def enable(self) -> pulumi.Output[_builtins.bool]:
        return pulumi.get(self, "enable")

    @_builtins.property
    @pulumi.getter(name="engineVersion")
    def engine_version(self) -> pulumi.Output[_builtins.float]:
        return pulumi.get(self, "engine_version")

    @_builtins.property
    @pulumi.getter(name="hybridSearchEnabled")
    def hybrid_search_enabled(self) -> pulumi.Output[_builtins.bool]:
        return pulumi.get(self, "hybrid_search_enabled")

    @_builtins.property
    @pulumi.getter(name="internalId")
    def internal_id(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "internal_id")

    @_builtins.property
    @pulumi.getter(name="lastActivity")
    def last_activity(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "last_activity")

    @_builtins.property
    @pulumi.getter(name="maxNumResults")
    def max_num_results(self) -> pulumi.Output[_builtins.int]:
        return pulumi.get(self, "max_num_results")

    @_builtins.property
    @pulumi.getter
    def metadata(self) -> pulumi.Output[Optional['outputs.AiSearchInstanceMetadata']]:
        return pulumi.get(self, "metadata")

    @_builtins.property
    @pulumi.getter(name="modifiedAt")
    def modified_at(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "modified_at")

    @_builtins.property
    @pulumi.getter(name="modifiedBy")
    def modified_by(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "modified_by")

    @_builtins.property
    @pulumi.getter
    def paused(self) -> pulumi.Output[_builtins.bool]:
        return pulumi.get(self, "paused")

    @_builtins.property
    @pulumi.getter(name="publicEndpointId")
    def public_endpoint_id(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "public_endpoint_id")

    @_builtins.property
    @pulumi.getter(name="publicEndpointParams")
    def public_endpoint_params(self) -> pulumi.Output['outputs.AiSearchInstancePublicEndpointParams']:
        return pulumi.get(self, "public_endpoint_params")

    @_builtins.property
    @pulumi.getter
    def reranking(self) -> pulumi.Output[_builtins.bool]:
        return pulumi.get(self, "reranking")

    @_builtins.property
    @pulumi.getter(name="rerankingModel")
    def reranking_model(self) -> pulumi.Output[Optional[_builtins.str]]:
        """
        Available values: "@cf/baai/bge-reranker-base", "".
        """
        return pulumi.get(self, "reranking_model")

    @_builtins.property
    @pulumi.getter(name="rewriteModel")
    def rewrite_model(self) -> pulumi.Output[Optional[_builtins.str]]:
        """
        Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        return pulumi.get(self, "rewrite_model")

    @_builtins.property
    @pulumi.getter(name="rewriteQuery")
    def rewrite_query(self) -> pulumi.Output[_builtins.bool]:
        return pulumi.get(self, "rewrite_query")

    @_builtins.property
    @pulumi.getter(name="scoreThreshold")
    def score_threshold(self) -> pulumi.Output[_builtins.float]:
        return pulumi.get(self, "score_threshold")

    @_builtins.property
    @pulumi.getter
    def source(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "source")

    @_builtins.property
    @pulumi.getter(name="sourceParams")
    def source_params(self) -> pulumi.Output['outputs.AiSearchInstanceSourceParams']:
        return pulumi.get(self, "source_params")

    @_builtins.property
    @pulumi.getter
    def status(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "status")

    @_builtins.property
    @pulumi.getter
    def summarization(self) -> pulumi.Output[_builtins.bool]:
        return pulumi.get(self, "summarization")

    @_builtins.property
    @pulumi.getter(name="summarizationModel")
    def summarization_model(self) -> pulumi.Output[Optional[_builtins.str]]:
        """
        Available values: "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "@cf/meta/llama-3.1-8b-instruct-fast", "@cf/meta/llama-3.1-8b-instruct-fp8", "@cf/meta/llama-4-scout-17b-16e-instruct", "@cf/qwen/qwen3-30b-a3b-fp8", "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b", "@cf/moonshotai/kimi-k2-instruct", "anthropic/claude-3-7-sonnet", "anthropic/claude-sonnet-4", "anthropic/claude-opus-4", "anthropic/claude-3-5-haiku", "cerebras/qwen-3-235b-a22b-instruct", "cerebras/qwen-3-235b-a22b-thinking", "cerebras/llama-3.3-70b", "cerebras/llama-4-maverick-17b-128e-instruct", "cerebras/llama-4-scout-17b-16e-instruct", "cerebras/gpt-oss-120b", "google-ai-studio/gemini-2.5-flash", "google-ai-studio/gemini-2.5-pro", "grok/grok-4", "groq/llama-3.3-70b-versatile", "groq/llama-3.1-8b-instant", "openai/gpt-5", "openai/gpt-5-mini", "openai/gpt-5-nano", "".
        """
        return pulumi.get(self, "summarization_model")

    @_builtins.property
    @pulumi.getter(name="systemPromptAisearch")
    def system_prompt_aisearch(self) -> pulumi.Output[Optional[_builtins.str]]:
        return pulumi.get(self, "system_prompt_aisearch")

    @_builtins.property
    @pulumi.getter(name="systemPromptIndexSummarization")
    def system_prompt_index_summarization(self) -> pulumi.Output[Optional[_builtins.str]]:
        return pulumi.get(self, "system_prompt_index_summarization")

    @_builtins.property
    @pulumi.getter(name="systemPromptRewriteQuery")
    def system_prompt_rewrite_query(self) -> pulumi.Output[Optional[_builtins.str]]:
        return pulumi.get(self, "system_prompt_rewrite_query")

    @_builtins.property
    @pulumi.getter(name="tokenId")
    def token_id(self) -> pulumi.Output[Optional[_builtins.str]]:
        return pulumi.get(self, "token_id")

    @_builtins.property
    @pulumi.getter
    def type(self) -> pulumi.Output[_builtins.str]:
        """
        Available values: "r2", "web-crawler".
        """
        return pulumi.get(self, "type")

    @_builtins.property
    @pulumi.getter(name="vectorizeActiveNamespace")
    def vectorize_active_namespace(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "vectorize_active_namespace")

    @_builtins.property
    @pulumi.getter(name="vectorizeName")
    def vectorize_name(self) -> pulumi.Output[_builtins.str]:
        return pulumi.get(self, "vectorize_name")

